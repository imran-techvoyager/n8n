import type { INodeType, INodeTypeDescription } from "../../../types";
import { Icon } from "./icon";

// function errorDescriptionMapper(error: NodeError) {
//   if (
//     error.description?.includes(
//       "properties: should be non-empty for OBJECT type"
//     )
//   ) {
//     return 'Google Gemini requires at least one <a href="https://docs.n8n.io/advanced-ai/examples/using-the-fromai-function/" target="_blank">dynamic parameter</a> when using tools';
//   }

//   return error.description ?? "Unknown error";
// }

export class LmChatGoogleGemini implements INodeType {
  description: INodeTypeDescription = {
    displayName: "Google Gemini Chat Model",

    name: "lmChatGoogleGemini",
    icon: {
      type: "component",
      component: Icon,
    },
    group: ["transform", "model"],
    nodeType: "chat-model", // it is not addded in the type yet
    version: 1,
    description: "Chat Model Google Gemini",
    defaults: {
      name: "Google Gemini Chat Model",
    },
    // codex: {
    // 	categories: ['AI'],
    // 	subcategories: {
    // 		AI: ['Language Models', 'Root Nodes'],
    // 		'Language Models': ['Chat Models (Recommended)'],
    // 	},
    // 	resources: {
    // 		primaryDocumentation: [
    // 			{
    // 				url: 'https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatgooglegemini/',
    // 			},
    // 		],
    // 	},
    // },

    // inputs: [],

    // outputs: [NodeConnectionTypes.AiLanguageModel],
    outputNames: ["Model"],
    credentials: [
      {
        name: "googleGeminiApi",
        required: true,
      },
    ],
    // requestDefaults: {
    // 	ignoreHttpStatusErrors: true,
    // 	baseURL: '={{ $credentials.host }}',
    // },
    properties: [
      // getConnectionHintNoticeField([NodeConnectionTypes.AiChain, NodeConnectionTypes.AiAgent]),
      {
        displayName: "Model",
        name: "modelName",
        type: "options",
        description:
          'The model which will generate the completion. <a href="https://developers.generativeai.google/api/rest/generativelanguage/models/list">Learn more</a>.',
        typeOptions: {
          // loadOptions: {
          // 	routing: {
          // 		request: {
          // 			method: 'GET',
          // 			url: '/v1beta/models',
          // 		},
          // 		output: {
          // 			postReceive: [
          // 				{
          // 					type: 'rootProperty',
          // 					properties: {
          // 						property: 'models',
          // 					},
          // 				},
          // 				{
          // 					type: 'filter',
          // 					properties: {
          // 						pass: "={{ !$responseItem.name.includes('embedding') }}",
          // 					},
          // 				},
          // 				{
          // 					type: 'setKeyValue',
          // 					properties: {
          // 						name: '={{$responseItem.name}}',
          // 						value: '={{$responseItem.name}}',
          // 						description: '={{$responseItem.description}}',
          // 					},
          // 				},
          // 				{
          // 					type: 'sort',
          // 					properties: {
          // 						key: 'name',
          // 					},
          // 				},
          // 			],
          // 		},
          // 	},
          // },
        },

        options: [
          {
            name: "models/gemini-2.5-flash",
            value: "models/gemini-2.5-flash",
            description:
              "Stable version of Gemini 2.5 Flash, our mid-size multimodal model that supports up to 1 million tokens, released in June of 2025.",
            action: "Delete a chat message",
          },
        ],
        // routing: {
        // 	send: {
        // 		type: 'body',
        // 		property: 'model',
        // 	},
        // },
        default: "models/gemini-2.5-flash",
      },
    ],
  };

  async supplyData({
    parameters,
    credentialId,
  }: {
    parameters: any;
    credentialId?: string;
  }): Promise<{ success: boolean; response?: any; error?: string }> {
    try {
      console.log("Supplying Google Gemini model with parameters:", parameters);

      const {
        modelName = "gemini-2.5-flash",
        temperature = 0.1,
        maxOutputTokens = 1000,
      } = parameters;

      // This supplies the model configuration that the agent will use
      const modelInstance = {
        provider: "google-gemini",
        model: modelName,
        temperature,
        maxOutputTokens,
        credentialId,

        // Method that agent can call to generate responses
        async generateResponse(prompt: string) {
          console.log(
            `Generating response with ${modelName} for prompt:`,
            prompt
          );

          // # todo: Have to implement real model calling using vercel ai sdk
          const response = `Generated by ${modelName}: ${prompt}`;

          return {
            success: true,
            response,
            model: modelName,
            usage: {
              promptTokens: prompt.length / 4, // rough estimate
              completionTokens: response.length / 4,
              totalTokens: (prompt.length + response.length) / 4,
            },
          };
        },
      };

      return {
        success: true,
        response: modelInstance,
      };
    } catch (error) {
      console.error("Gemini model supply error:", error);

      return {
        success: false,
        error:
          error instanceof Error
            ? error.message
            : "Unknown error occurred in model supply",
      };
    }
  }

  // Fallback execute method in case model node gets executed directly
  async execute(parameters: any, credentialId?: string) {
    console.warn(
      "Model node executed directly - models should be connected to agent's sub-component handles"
    );

    return {
      success: true,
      data: {
        message:
          "Model node should be connected to agent node's bottom handles, not executed directly",
        modelName: parameters.parameters?.modelName || "gemini-1.5-flash",
      },
    };
  }
}
